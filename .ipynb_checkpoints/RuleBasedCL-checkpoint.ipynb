{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import csv\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is a rule based classfier to detect quotes and lyrics based on the names and content of the website, the evaluation of the classifer is in MLClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'all.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b6277780a609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'all.csv' does not exist"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv('all.csv')\n",
    "file[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39041\n"
     ]
    }
   ],
   "source": [
    "print(len(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess data: the preprocess here is just lower case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize\n",
    "def preprocess(sent):\n",
    "\n",
    "    words = str(sent).lower().split()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        w = re.sub(r'[0-9]+', '', w)\n",
    "        new_words.append(w)\n",
    "        \n",
    "    return ' '.join(new_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Here we create a class to store each variable as an object \n",
    "2. Return the count of key words 'lyric, lyrics, quote, quotes' in the name of the website, because the name of the website contains most of the information we need\n",
    "3. Count the keywords and store it as Score. Score is a vector that contain keyword counts in each search result\n",
    "4. compute cosine similarity between post and retrieve website content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class myQuote:\n",
    "    def __init__(self, text):\n",
    "        #read in CSV, process\n",
    "        self.quoteText = text\n",
    "        self.quoteID = hash(self.quoteText)\n",
    "        self.link = []\n",
    "        self.name = []\n",
    "        self.description = []\n",
    "        self.scores = []\n",
    "        self.cos = []\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return self.quoteID\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Object text: \" + self.quoteText + '\\n' +\\\n",
    "        \"Links: \" + str(self.link) + '\\n' +\\\n",
    "        \"Names: \" + str(self.name) + '\\n' +\\\n",
    "        \"Description \" + str(self.description) + '\\n' +\\\n",
    "        \"Scores: \" + str(self.scores)\n",
    "        \n",
    "\n",
    "objects = {}\n",
    "with open('all_multiCos.csv', 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        #print(row['text'])\n",
    "        texthash = hash(row['text'])\n",
    "        if texthash not in objects:\n",
    "            objects[texthash] = myQuote(row['text'])\n",
    "        objects[texthash].link.append(row['link'])\n",
    "        objects[texthash].name.append(row['name'])\n",
    "        objects[texthash].description.append(row['description'])\n",
    "        line = preprocess(row['name'])\n",
    "        #count keywords\n",
    "        count = line.count(\"lyric\") + line.count(\"lyrics\") + line.count(\"quote\") + line.count(\"quotes\")\n",
    "        objects[texthash].scores.append(count)\n",
    "        str1 = ''.join (objects[texthash].description)\n",
    "        doc2 = nlp(preprocess(str1))\n",
    "        doc1 = nlp(preprocess(objects[texthash].quoteText))\n",
    "        objects[texthash].cos.append(doc1.similarity(doc2))\n",
    "  \n",
    "        #add extra fields in the class if I need to add more fields        \n",
    "#print(len(objects.keys()))\n",
    "#for item in objects.keys():\n",
    "#    if sum(objects[item].scores) > 0:\n",
    "#        print(objects[item])\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add cosine similarity as an object\n",
    "#remove punctionations\n",
    "def preprocess2(sent):\n",
    "    #remove punctustion\n",
    "    sent = re.sub(r'[^\\w\\s]','',sent)\n",
    "    words = sent.split()\n",
    "    new_words = []\n",
    "    for w in words:      \n",
    "        new_words.append(w.lower())\n",
    "        \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "s = '? And now I realise//I should\\'ve kissed you in LA//But I drove home all alone//As if I had a choice anyway~ ?'\n",
    "preprocess2(s)\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "def cosineSim(results):\n",
    "    for item in results:\n",
    "        str1 = ''.join (objects[item].description)\n",
    "        doc2 = nlp(preprocess2(str1))\n",
    "        doc1 = nlp(preprocess2(objects[item].quoteText))\n",
    "        objects[item].cos.append(doc1.similarity(doc2))\n",
    "\n",
    "cosineSim(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here we save the file as csv, the csv file is just for your reference\n",
    "def saveAsCSV(dictz,filename):\n",
    "    f = open(filename, 'w')\n",
    "    writer = csv.writer(f, delimiter = ',',quoting=csv.QUOTE_MINIMAL)  \n",
    "    writer.writerow([\"hash\"] + [\"text\"] + [\"count\"] + [\"cosineSim\"])\n",
    "    for item in objects.keys():\n",
    "        writer.writerow([objects[item].quoteID] + [objects[item].quoteText] + [objects[item].scores] + [objects[item].cos])\n",
    "    f.close()\n",
    "    \n",
    "saveAsCSV(objects, 'myprocessedquotes2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  compute cosine similarity between posts and content (description) of the search result, those with 90% above similarity are labeled as quote. Those less than 92% are labeled as suspect.\n",
    "2. cosine similarity >= 96%, label it as quote\n",
    "3. cosine similarity > 92% but < 96% \n",
    "    1. Count keywords in search results (link names), if result contains keywords in  more than two document, label it as quote\n",
    "    2. otherwise label it as suspect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model:\n",
    "In the evaluation set\n",
    "Among 200 posts: 24 suspect, 25 quotes\n",
    "4 in 24 are actual quotes\n",
    "2 among the rest is actual quote\n",
    "\n",
    "Precision: 25/25 = 1\n",
    "Recall: 25/31 = 0.81\n",
    "F = 2(1*0.81/1+0.81) = 0.895"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use cosine similarity to compare text with search result\n",
    "nlp = spacy.load('en')\n",
    "def SearchResult2(results, filename):\n",
    "    f = open(filename, 'w')\n",
    "    writer = csv.writer(f, delimiter = ',',quoting=csv.QUOTE_MINIMAL)  \n",
    "    writer.writerow([\"hash\"] + [\"text\"] + [\"count\"]+ [\"cosineSim\"] +['label'])\n",
    "    for item in results:\n",
    "        if objects[item].cos[0] < 0.92:\n",
    "            writer.writerow([objects[item].quoteID] + [objects[item].quoteText] + [objects[item].scores] + [objects[item].cos[0]]+['NotQuote'])\n",
    "        elif objects[item].cos[0] > 0.92 and objects[item].cos[0] < 0.96:\n",
    "            count = 0\n",
    "            for score in objects[item].scores:\n",
    "                if score > 1: # 1 is 0 before we add 1 for smoothing\n",
    "                    count = count + 1\n",
    "            if count >= 2 :\n",
    "                writer.writerow([objects[item].quoteID] + [objects[item].quoteText] + [objects[item].scores] + [objects[item].cos[0]]+['quote'])\n",
    "            else:\n",
    "                writer.writerow([objects[item].quoteID] + [objects[item].quoteText] + [objects[item].scores] + [objects[item].cos[0]]+['suspect'])\n",
    "        else:\n",
    "            writer.writerow([objects[item].quoteID] + [objects[item].quoteText] + [objects[item].scores] + [objects[item].cos[0]]+['quote'])\n",
    "   # else:\n",
    "       # writer.writerow([objects[item].quoteID] + [objects[item].quoteText] + [objects[item].scores] + ['null']+['NotQuote'])\n",
    "    f.close()\n",
    "        \n",
    "results = objects.keys()\n",
    "SearchResult2(results, 'QuotesDetected_all3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing features for ML models:\n",
    "Not all the queries have the same number of results, now we align the result vector as 10, which means we packed the result vector to 10 with 0. 10 is the maxium results in the entries. \n",
    "But before we algin the vector, we smooth the score results by adding 1, then the empty result is represented by 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def align_scores(results):\n",
    "    for item in results:\n",
    "        count = 0\n",
    "        for i in range(len(objects[item].scores)): # add 1 to all the scores \n",
    "            objects[item].scores[i] = objects[item].scores[i] +1           \n",
    "            if objects[item].scores[i] is not None:\n",
    "                count = count + 1 \n",
    "               # score = score + 1 won't work because score is a shallow copy\n",
    "        #print(objects[item].scores)\n",
    "        while count < 10:\n",
    "            objects[item].scores.append(0) #empty result is replaced by 0\n",
    "            count = count + 1\n",
    "\n",
    "align_scores(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 5, 7, 5, 7, 1, 1, 6, 0]\n",
      "[5, 2, 3, 1, 1, 1, 1, 1, 3, 0]\n",
      "[5, 7, 3, 5, 9, 9, 5, 1, 1, 0]\n",
      "[1, 1, 1, 1, 3, 1, 1, 1, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "#now let's see the aligned vectors\n",
    "count = 0\n",
    "for item in results:\n",
    "    print(objects[item].scores)\n",
    "    count = count + 1\n",
    "    if count > 3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function prints the result from the model and the algined vectors\n",
    "def printFeatures(results, filename):\n",
    "    SearchResult2(results, 'temp.csv')\n",
    "        \n",
    "printFeatures(results, 'features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump object to pickles\n",
    "import pickle\n",
    "\n",
    "filename = 'searchResults'\n",
    "outfile = open(filename,'wb')\n",
    "\n",
    "pickle.dump(objects,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
